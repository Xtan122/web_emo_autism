import os
import cv2
import torch
import numpy as np
import base64
from flask import Flask, request, jsonify
from flask_cors import CORS
from torchvision import transforms
from transformers import ViTForImageClassification
from PIL import Image

app = Flask(__name__)
CORS(app)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, 'ER_model')

YUNET_PATH = os.path.join(MODEL_DIR, 'face_detection_yunet_2023mar.onnx')
VIT_PATH = os.path.join(MODEL_DIR, 'vit_model.pth')


RAW_LABELS = [
    'dg', # 0: Genuine Disgust
    'ds', # 1: Posed Disgust
    'fg', # 2: Genuine Happiness
    'fs', # 3: Posed Happiness
    'pg', # 4: Genuine Fear
    'ps', # 5: Posed Fear
    'rg', # 6: Genuine Anger
    'rs', # 7: Posed Anger
    'sg', # 8: Genuine Surprise
    'ss', # 9: Posed Surprise
    'tg', # 10: Genuine Sadness
    'ts'  # 11: Posed Sadness
]

# Map t·ª´ m√£ t·∫Øt v·ªÅ t√™n ti·∫øng Anh chu·∫©n m√† Frontend VueJS ƒëang d√πng
MERGE_MAPPING = {
    'dg': 'disgust',   'ds': 'disgust',   # Gh√™ t·ªüm
    'fg': 'happy',     'fs': 'happy',     # Vui v·∫ª
    'pg': 'fear',      'ps': 'fear',      # S·ª£ h√£i
    'rg': 'angry',     'rs': 'angry',     # Gi·∫≠n d·ªØ
    'sg': 'surprised', 'ss': 'surprised', # Ng·∫°c nhi√™n
    'tg': 'sad',       'ts': 'sad'        # Bu·ªìn b√£
}

# --- 2. LOAD MODELS ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üîÑ ƒêang kh·ªüi ƒë·ªông tr√™n thi·∫øt b·ªã: {device}")

# Load Face Detector
try:
    face_detector = cv2.FaceDetectorYN.create(
        model=YUNET_PATH, config="", input_size=(320, 320),
        score_threshold=0.8, nms_threshold=0.3, top_k=5000
    )
    print("‚úÖ YuNet loaded successfully!")
except Exception as e:
    print(f"‚ùå L·ªói load YuNet: {e}")

try:
    # Kh·ªüi t·∫°o model v·ªõi 12 ƒë·∫ßu ra
    base_model = ViTForImageClassification.from_pretrained(
        'google/vit-base-patch16-224',
        num_labels=12,
        ignore_mismatched_sizes=True
    )
    
    # Load tr·ªçng s·ªë
    state_dict = torch.load(VIT_PATH, map_location=device)
    base_model.load_state_dict(state_dict)
    
    # Wrapper class
    class ViT_Wrapper(torch.nn.Module):
        def __init__(self, hf_model):
            super().__init__()
            self.vit = hf_model.vit
            self.classifier = hf_model.classifier
        def forward(self, x):
            outputs = self.vit(x)
            cls_output = outputs.last_hidden_state[:, 0, :]
            return self.classifier(cls_output)

    model = ViT_Wrapper(base_model)
    model.to(device)
    model.eval()
    print("‚úÖ ViT Emotion Model (12 Classes) loaded successfully!")

except Exception as e:
    print(f"‚ùå L·ªói load ViT: {e}")

# Transform ·∫£nh
val_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

@app.route('/analyze', methods=['POST'])
def analyze():
    if 'image' not in request.json:
        return jsonify({"status": "error", "message": "No image data"}), 400

    try:
        # Decode ·∫£nh
        image_data = request.json['image']
        if "," in image_data: image_data = image_data.split(',')[1]
        
        image_bytes = base64.b64decode(image_data)
        np_arr = np.frombuffer(image_bytes, np.uint8)
        img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)

        # Detect khu√¥n m·∫∑t
        h, w, _ = img.shape
        face_detector.setInputSize((w, h))
        _, faces = face_detector.detect(img)

        if faces is None or len(faces) == 0:
            return jsonify({"status": "no_face", "emotions": {}})

        # Crop khu√¥n m·∫∑t
        box = faces[0][:4].astype(int)
        x, y, w_box, h_box = box
        
        # Crop an to√†n
        x1, y1 = max(0, x), max(0, y)
        x2, y2 = min(w, x + w_box), min(h, y + h_box)
        face_crop = img[y1:y2, x1:x2]

        if face_crop.size == 0:
            return jsonify({"status": "error"})

        # D·ª± ƒëo√°n (12 Classes)
        face_pil = Image.fromarray(cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB))
        input_tensor = val_transforms(face_pil).unsqueeze(0).to(device)

        with torch.no_grad():
            outputs = model(input_tensor)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            probs_np = probs.cpu().numpy()[0]

        # --- G·ªòP 12 NH√ÉN V·ªÄ 6 NH√ÉN ---
        final_scores = {
            'happy': 0.0, 'sad': 0.0, 'fear': 0.0, 
            'angry': 0.0, 'surprised': 0.0, 'disgust': 0.0
        }

        for i, raw_score in enumerate(probs_np):
            raw_label = RAW_LABELS[i]
            target_label = MERGE_MAPPING.get(raw_label)
            if target_label:
                final_scores[target_label] += raw_score

        # Chuy·ªÉn v·ªÅ % (Integer)
        response_emotions = {k: int(v * 100) for k, v in final_scores.items()}
        dominant_emotion = max(response_emotions, key=response_emotions.get)

        # Log ki·ªÉm tra
        print(f"Detected: {dominant_emotion} (Raw: {response_emotions})")

        return jsonify({
            "status": "success",
            "dominant": dominant_emotion,
            "emotions": response_emotions
        })

    except Exception as e:
        print(f"Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)